{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW06: Word Embeddings\n",
    "\n",
    "Remember that these homework work as a completion grade. **In this homework, we present two tasks and you can choose which one you want to solve. You only have to solve <span style=\"color:red\">one task</span> in this homework.**\n",
    "Task 1 is more guided and we evaluate document embeddings on a standard benchmark. Task 2 is very open-end and might be a starting point for your course project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "In this task, we evaluate different document embeddings on the English version of the [STS Benchmark](https://arxiv.org/pdf/1708.00055.pdf). The task is to determine how semantically similar two texts are and is a popular dataset to evaluate document embeddings, i.e. we want embeddings of two semantically similar documents to be similar as well. We provide a wordcounts baseline for this task and ask you to compute and evaluate embeddings for a selected sample of document embedding techniques.\n",
    "\n",
    "To evaluate, we follow [(Reimers and Gurevych, 2019)](https://arxiv.org/pdf/1908.10084.pdf) and compute the Spearmanâ€™s rank correlation between the cosine-similarity of thesentence embeddings and the gold labels. **It is ok to skip one of the document embedding methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the data\n",
    "# !wget http://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.eval.v1.1.zip\n",
    "# !wget http://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.gs.zip\n",
    "#\n",
    "# !unzip sts2017.eval.v1.1.zip\n",
    "# !unzip sts2017.gs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "('A person is on a baseball team.',\n 'A person is playing basketball on a team.',\n 2.4)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "def load_STS_data():\n",
    "    with open(\"STS2017.gs/STS.gs.track5.en-en.txt\") as f:\n",
    "        labels = [float(line.strip()) for line in f]\n",
    "    \n",
    "    text_a, text_b = [], []\n",
    "    with open(\"STS2017.eval.v1.1/STS.input.track5.en-en.txt\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            text_a.append(line[0])\n",
    "            text_b.append(line[1])\n",
    "    return text_a, text_b, labels\n",
    "\n",
    "text_a, text_b, labels = load_STS_data()\n",
    "text_a[0], text_b[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utils\n",
    "from scipy.stats import spearmanr\n",
    "def evaluate(predictions, labels):\n",
    "    print (\"spearman's rank correlation\", spearmanr(predictions, labels)[0])\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation 0.6998056665685976\n"
     ]
    }
   ],
   "source": [
    "# Wordcounts baseline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "vec.fit(text_a + text_b)\n",
    "\n",
    "# encode documents\n",
    "text_a_encoded = np.array(vec.transform(text_a).todense())\n",
    "text_b_encoded = np.array(vec.transform(text_b).todense())\n",
    "\n",
    "# predict cosine similarities\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(text_a_encoded, text_b_encoded)]\n",
    "\n",
    "# evaluate\n",
    "evaluate(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO train Doc2Vec on the texts in the dataset\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk import word_tokenize\n",
    "from random import shuffle\n",
    "\n",
    "docs = []\n",
    "all_texts = [*text_a, *text_b]\n",
    "for text in all_texts:\n",
    "    docs += [word_tokenize(text)]\n",
    "shuffle(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "doc_iterator = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "d2v = Doc2Vec(doc_iterator,\n",
    "                min_count=10, # minimum word count\n",
    "                window=10,    # window size\n",
    "                vector_size=100, # size of document vector\n",
    "                sample=1e-4,\n",
    "                negative=5,\n",
    "                workers=4, # threads\n",
    "                max_vocab_size=1000) # max vocab size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "##TODO derive the word vectors for each text in the dataset\n",
    "wvs_a = [d2v.infer_vector([doc]) for doc in text_a]\n",
    "wvs_b = [d2v.infer_vector([doc]) for doc in text_b]\n",
    "\n",
    "# wvs_a = d2v.infer_vector(text_a)\n",
    "# wvs_b = d2v.infer_vector(text_b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation -0.028520686503480268\n"
     ]
    }
   ],
   "source": [
    "##TODO compute cosine similarity between the text pairs and evaluate spearman's rank correlation\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(wvs_a, wvs_b)]\n",
    "evaluate(predictions, labels)\n",
    "\n",
    "\n",
    "## Don't worry if results are not satisfactory using Doc2Vec (the dataset is too small to train good embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation 0.5210910398082091\n"
     ]
    }
   ],
   "source": [
    "##TODO do the same with embeddings provided by spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "wvs_a = [nlp(doc).vector for doc in text_a]\n",
    "wvs_b = [nlp(doc).vector for doc in text_b]\n",
    "\n",
    "\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(wvs_a, wvs_b)]\n",
    "evaluate(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n",
      "spearman's rank correlation 0.8493103413219787\n"
     ]
    }
   ],
   "source": [
    "##TODO do the same with universal sentence embeddings\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "    return model(input)\n",
    "\n",
    "# wvs_a = [embed([text]) for text in text_a]\n",
    "# wvs_b = [embed([text]) for text in text_b]\n",
    "\n",
    "embed_a = embed(text_a)\n",
    "embed_b = embed(text_b)\n",
    "\n",
    "\n",
    "# for a, b in zip(text_a):\n",
    "#     embed_a = embed([text])\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    wvs_a = session.run(embed_a)\n",
    "    wvs_b = session.run(embed_b)\n",
    "\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(wvs_a, wvs_b)]\n",
    "evaluate(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0.00/405M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1865bcfb92234e4b834e9f5de5de0292"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman's rank correlation 0.8008164100246977\n"
     ]
    }
   ],
   "source": [
    "##TODO do the same with SBERT embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = \"bert-base-nli-mean-tokens\"\n",
    "embedder = SentenceTransformer(model)\n",
    "wvs_a = embedder.encode(text_a)\n",
    "wvs_b = embedder.encode(text_b)\n",
    "\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(wvs_a, wvs_b)]\n",
    "evaluate(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "Use your favorite document embeddings method to compute embeddings for a dataset you are interested in. Think of a method and provide some data visualization statistics (one method would be the path we have chosen in the notebook, i.e. cluster the embeddings with k-means and visualize low-dimensional representations of the document embeddings obtained by PCA). \n",
    "\n",
    "This task is very open and there is no right or wrong; If you want to use document embeddings in your course project, this is a chance to play around with them.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}